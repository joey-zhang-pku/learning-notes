# Wednesday  

## Lecture 26: Clustering  

Partitional approaches: K-Means Clustering  
K-Means: Clustering  
K-Nearest Neighbors: Classification  

Different starts point, different output!  
Loss function?  
Inertia: Sum of squared distances from each data point to its center  
Distortion: Weighted sum of squared distances from each data point to its center.  
Minimize the inertia.  
Can think of K-means as a pair of optimizers that take turns:  
Hold cluster centers constant, optimize colors; Hold colors constant, optimize centers.  
NP-hard to find the global inertia given # colors K.  
Algorithm:  
For all possible k**n colorings, compute the minimize inertia.  

Hierarchical Agglomerative Clustering:  
Linkage criterion: Single, Average, Complete  
Dendrogram: Visualize the merging hierarchy.  

Pick K: Subjective  

Plot inertia versus many different K values  
Use a dendrogram  

Sihouette Scores:  
High score: Clsoe to points in its own cluster. Far from points in other clusters.  
Low score: Vice versa.  

A = Mean distance to points in same cluster  
B = Mean distance to points in nearest cluster.  
S = (B-A)/max(A,B)  
value of S can be negative!  

Evaluate Clustering: Average Sihouette Score/ Inertia...  








