# Tuesday

## Lecture 21: SQL II  

## Lecture 22: Logistic Regression I

### Regression vs. Classification  
Confusing language: We use logistic regression to perform classification.

Taxonomy of Machine Learning:

Labeled data--- Supervised Learning --- Regression/ Classification 

Unlabeled data--- Unsupervised Learning --- Dimensionality Reduction/ Clustering  

Typical workflow:  
1.Choose a model  
2.Choose a loss function  
3.Fit the model: Regularization Sklearn/Gradient descent
4.Evaluate model performance  

Estimate P(Y=1|bin) (come from data8 idea, calculate the mean within a point's neighbors): use sigmoid function  

Like an linear regression wrapped with sigmoid!  


To simplify, trying to isolate the linear combination part, we have:  
theta0 + theta1*x = log(P/(1-P))  

Deicision Rule T:  
T, the probability threshold, is often chosen to be T, be not always.  

Cross-Entropy Loss:  

Non-convex: MSE is not comvex for this model, which means our gradient descent may fail.  
Bounded: For binary classfication, L2 Loss is capped at 1.  

We deinfe CE-Loss(Aim: if true_y=1, we're going to penalize small P, and vice versa):  
CE loss= -logp if y=1; -log(1-p) if y=0.  

A little bit tricky, that's equal to one expression:  

-(y*log(p) + (1-y)*llog(1-p))  

Then it's convex! Gradient descent works  

Bonus: Maximum Likelihood Estimation:  

Our MLE problem:  
1.For i =1,2,...,n, let Yi be independent Bernoulli(pi). Observe data {y1,y2,...yn}  
2.We'd like to estiimate p1,p2,...,pn.

Our estimation probabilities should maximize the probability of the given outcome!  

Conclusion:  
Minimizing croee-entropy loss is equivalent to maximizing the likelihood of the training data!  
MLE can explain and motivate many model+loss combiantions!  

## Lecture 23: Logostic Regression II  

Linear Separability:  
A classification datasets is said to be linearly separable if there exists a hyperplane among input features x that separates the two classes y.  

When the dataset is linearly separable, the logistic regression classifier can perfectly assign data into classes, BUT can not achieve 0 cross-entropy loss.  

When data islinearly separable, the potimal model arameters diverge to +-infinite.  
Grdient descent follows the tilted loss surface downwards, it never converges.  

A model with infinite parameters is overconfident, which means the test loss might be infinite! (train: (1,1), test: (-0.5,1))  

Regularization: to avoid large weights( particularly on linearly separable data)  


Performance Metrics:  
Motivation: CE-Loss is not interpretable!  

Accuracy's pitfall: class imbalance  

accuracy = (TP+TN)/n  
precision = TP/P (penalizes FPs)  
recall = TP/TP+FN (penalizes FNs)  

Precision-Recalll Curves:  
Negative association; sweet point while balancing the costs of FP and FN!  

F1 Score:  
harmonic mean of precision and recall.  

ROC Curves:  
FPR: False Posotive Rate = FP/(FP+TN)  
TPR: True Positive Rate = TP/(TP+FN)  

Best area under the ROC curve is 1.  
A predictor that predicts ranodmly has a n AUC-ROC of 0.5.  

Difference between AUC-ROC and PR-AUC:  
balance? focus on positive? ...





